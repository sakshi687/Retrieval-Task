{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Corpus From Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "folder = \"C:\\\\Users\\\\ASUS\\\\AILA Practice\\\\casedocs_idf\"\n",
    "os.chdir(folder)\n",
    "files = glob.glob(\"*.txt\") # Makes a list of all files in folder\n",
    "corpus = []\n",
    "corpus_dict = {}\n",
    "for file1 in files:\n",
    "    with open (file1, 'r') as f:\n",
    "        document = f.read() # Reads document content into a string\n",
    "        corpus.append(document)\n",
    "        corpus_dict[file1] = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify named entities using spaCy library and remove them from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_strings(cur_string, replace_list):\n",
    "  for cur_word in replace_list:\n",
    "    cur_string = cur_string.replace(cur_word, '', 1)\n",
    "  return cur_string.lower()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "for doc in new_corpus:\n",
    "    i = new_corpus.index(doc)\n",
    "    processed_doc = nlp(doc)\n",
    "    entities = []\n",
    "    for ent in processed_doc.ents:\n",
    "        entities.append(ent.text)\n",
    "    new_corpus[i] = remove_multiple_strings(doc, entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the corpus into tokens and remove puntuations and stop words using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = []\n",
    "for text in new_corpus:\n",
    "    doc2 = nlp(text)\n",
    "    tokens_in_one_corpus = []\n",
    "    for token in doc2:\n",
    "        if not token.is_punct | token.is_space | token.is_stop :\n",
    "            tokens_in_one_corpus.append(token.orth_ )\n",
    "    tokenized_corpus.append(tokens_in_one_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------GENSIM GITHUB CODE START------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from six import iteritems\n",
    "from six.moves import range\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "PARAM_K1 = 1.5\n",
    "PARAM_B = 0.75\n",
    "EPSILON = 0.25\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    \"\"\"Implementation of Best Matching 25 ranking function.\n",
    "    Attributes\n",
    "    ----------\n",
    "    corpus_size : int\n",
    "        Size of corpus (number of documents).\n",
    "    avgdl : float\n",
    "        Average length of document in `corpus`.\n",
    "    doc_freqs : list of dicts of int\n",
    "        Dictionary with terms frequencies for each document in `corpus`. Words used as keys and frequencies as values.\n",
    "    idf : dict\n",
    "        Dictionary with inversed documents frequencies for whole `corpus`. Words used as keys and frequencies as values.\n",
    "    doc_len : list of int\n",
    "        List of document lengths.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus, k1=PARAM_K1, b=PARAM_B, epsilon=EPSILON):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : list of list of str\n",
    "            Given corpus.\n",
    "        k1 : float\n",
    "            Constant used for influencing the term frequency saturation. After saturation is reached, additional\n",
    "            presence for the term adds a significantly less additional score. According to [1]_, experiments suggest\n",
    "            that 1.2 < k1 < 2 yields reasonably good results, although the optimal value depends on factors such as\n",
    "            the type of documents or queries.\n",
    "        b : float\n",
    "            Constant used for influencing the effects of different document lengths relative to average document length.\n",
    "            When b is bigger, lengthier documents (compared to average) have more impact on its effect. According to\n",
    "            [1]_, experiments suggest that 0.5 < b < 0.8 yields reasonably good results, although the optimal value\n",
    "            depends on factors such as the type of documents or queries.\n",
    "        epsilon : float\n",
    "            Constant used as floor value for idf of a document in the corpus. When epsilon is positive, it restricts\n",
    "            negative idf values. Negative idf implies that adding a very common term to a document penalize the overall\n",
    "            score (with 'very common' meaning that it is present in more than half of the documents). That can be\n",
    "            undesirable as it means that an identical document would score less than an almost identical one (by\n",
    "            removing the referred term). Increasing epsilon above 0 raises the sense of how rare a word has to be (among\n",
    "            different documents) to receive an extra score.\n",
    "        \"\"\"\n",
    "\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.corpus_size = 0\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self._initialize(corpus)\n",
    "\n",
    "    def _initialize(self, corpus):\n",
    "        \"\"\"Calculates frequencies of terms in documents and in corpus. Also computes inverse document frequencies.\"\"\"\n",
    "        nd = {}  # word -> number of documents with word\n",
    "        num_doc = 0\n",
    "        for document in corpus:\n",
    "            self.corpus_size += 1\n",
    "            self.doc_len.append(len(document))\n",
    "            num_doc += len(document)\n",
    "\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.doc_freqs.append(frequencies)\n",
    "\n",
    "            for word, freq in iteritems(frequencies):\n",
    "                if word not in nd:\n",
    "                    nd[word] = 0\n",
    "                nd[word] += 1\n",
    "\n",
    "        self.avgdl = float(num_doc) / self.corpus_size\n",
    "        # collect idf sum to calculate an average idf for epsilon value\n",
    "        idf_sum = 0\n",
    "        # collect words with negative idf to set them a special epsilon value.\n",
    "        # idf can be negative if word is contained in more than half of documents\n",
    "        negative_idfs = []\n",
    "        for word, freq in iteritems(nd):\n",
    "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                negative_idfs.append(word)\n",
    "        self.average_idf = float(idf_sum) / len(self.idf)\n",
    "\n",
    "        if self.average_idf < 0:\n",
    "            logger.warning(\n",
    "                'Average inverse document frequency is less than zero. Your corpus of {} documents'\n",
    "                ' is either too small or it does not originate from natural text. BM25 may produce'\n",
    "                ' unintuitive results.'.format(self.corpus_size)\n",
    "            )\n",
    "\n",
    "        eps = self.epsilon * self.average_idf\n",
    "        for word in negative_idfs:\n",
    "            self.idf[word] = eps\n",
    "\n",
    "    def get_score(self, document, index):\n",
    "        \"\"\"Computes BM25 score of given `document` in relation to item of corpus selected by `index`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : list of str\n",
    "            Document to be scored.\n",
    "        index : int\n",
    "            Index of document in corpus selected to score with `document`.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            BM25 score.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        doc_freqs = self.doc_freqs[index]\n",
    "        numerator_constant = self.k1 + 1\n",
    "        denominator_constant = self.k1 * (1 - self.b + self.b * self.doc_len[index] / self.avgdl)\n",
    "        for word in document:\n",
    "            if word in doc_freqs:\n",
    "                df = self.doc_freqs[index][word]\n",
    "                idf = self.idf[word]\n",
    "                score += (idf * df * numerator_constant) / (df + denominator_constant)\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, document):\n",
    "        \"\"\"Computes and returns BM25 scores of given `document` in relation to\n",
    "        every item in corpus.\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : list of str\n",
    "            Document to be scored.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            BM25 scores.\n",
    "        \"\"\"\n",
    "        scores = [self.get_score(document, index) for index in range(self.corpus_size)]\n",
    "        return scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------GENSIM GITHUB CODE END--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying gensim BM25 to the corpus to obtain scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "i=1\n",
    "for i in range(len(corpus)):\n",
    "    list.append(tokenized_corpus[i+1]) \n",
    "p1 = BM25(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tokenized_corpus[0]\n",
    "resultant_scores = p1.get_scores(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting Resultant Sscores In Descending Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59.01691898,  56.21210348,  81.34112887, ...,  77.24745092,\n",
       "        64.99442445, 103.44346956])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "res_list = np.array(resultant_scores)\n",
    "res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = np.argsort(res_list)[::-1]\n",
    "result = [corpus[i] for i in top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([132.71668413, 120.39917881, 117.22699723, ...,  16.26248352,\n",
       "        15.82500033,  12.93721765])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = res_list[top_n]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Filename Corresponding To Sscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for item in result:\n",
    "    for filename, content in corpus_dict.items():\n",
    "        if content == item:\n",
    "            output.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = []\n",
    "sstring = '.txt'\n",
    "for item in output:\n",
    "    if item.endswith(sstring): \n",
    "        new_output.append(item[:-(len(sstring))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing To a File For TREC-EVAL Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"C:\\\\Users\\\\ASUS\\\\AILA Practice\\\\run-NER-bm25.txt\",\"a\")\n",
    "for i in range(len(corpus)):\n",
    "    f.write(\"AILA_Q28 Q0 {} {} {} Default\\n\".format(new_output[i], (i+1), scores[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
