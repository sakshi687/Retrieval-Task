{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPROVED BM25 APPROACH: \n",
    "relevance(q,d) = BM25(q',d) + BM25(q\",d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Corpus from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "folder = \"C:\\\\Users\\\\ASUS\\\\AILA Practice\\\\casedocs\"\n",
    "os.chdir(folder)\n",
    "files = glob.glob(\"*.txt\") # Makes a list of all files in folder\n",
    "corpus = []\n",
    "corpus_dict = {}\n",
    "for file1 in files:\n",
    "    with open (file1, 'r') as f:\n",
    "        document = f.read() # Reads document content into a string\n",
    "        corpus.append(document)\n",
    "        corpus_dict[file1] = document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations and stemming using Porter Stemmer and creating tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to Lowercase\n",
    "import nltk\n",
    "def to_lower(text):\n",
    "    return ' '.join([w.lower() for w in nltk.word_tokenize(text)])\n",
    "\n",
    "for doc in new_corpus:\n",
    "    i = new_corpus.index(doc)\n",
    "    result = to_lower(doc)\n",
    "    new_corpus[i] = result\n",
    "    \n",
    "#Remove Punctuations\n",
    "from string import punctuation\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "for doc in new_corpus:\n",
    "    i = new_corpus.index(doc)\n",
    "    result = strip_punctuation(doc)\n",
    "    new_corpus[i] = result\n",
    "\n",
    "#Remove Numbers\n",
    "import re\n",
    "for doc in new_corpus:\n",
    "    i = new_corpus.index(doc)\n",
    "    result = re.sub(r'\\b[\\d]+\\b', '', doc)\n",
    "    new_corpus[i] = result\n",
    "\n",
    "#Remove Stop Words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in new_corpus]\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    i = tokenized_corpus.index(doc)\n",
    "    result = [word for word in tokenized_corpus[i] if word not in stopwords]\n",
    "    tokenized_corpus[i] = result\n",
    "    \n",
    "#Porter Stemmer for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for docs in tokenized_corpus:\n",
    "    i = tokenized_corpus.index(docs)\n",
    "    for words in docs:\n",
    "        j = tokenized_corpus[i].index(words)\n",
    "        tokenized_corpus[i][j] = ps.stem(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using rank_bm25 library and running it against tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25A = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q' = FINDING IDF VALUES FOR QUERY AND EXTRACTING TOP 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing casedocs_idf = casedocs + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "folder_idf = \"C:\\\\Users\\\\ASUS\\\\AILA Practice\\\\casedocs_idf\"\n",
    "os.chdir(folder_idf)\n",
    "files_idf = glob.glob(\"*.txt\") # Makes a list of all files in folder\n",
    "corpus_idf = []\n",
    "dict_idf={}\n",
    "for file1 in files_idf:\n",
    "    with open (file1, 'r') as f:\n",
    "        document = f.read() # Reads document content into a string\n",
    "        corpus_idf.append(document)\n",
    "        dict_idf[file1] = document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing it the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to Lowercase\n",
    "import nltk\n",
    "def to_lower(text):\n",
    "    return ' '.join([w.lower() for w in nltk.word_tokenize(text)])\n",
    "\n",
    "for doc in corpus_idf:\n",
    "    i = corpus_idf.index(doc)\n",
    "    result = to_lower(doc)\n",
    "    corpus_idf[i] = result\n",
    "    \n",
    "#Remove Punctuations\n",
    "from string import punctuation\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "for doc in corpus_idf:\n",
    "    i = corpus_idf.index(doc)\n",
    "    result = strip_punctuation(doc)\n",
    "    corpus_idf[i] = result\n",
    "\n",
    "#Remove Numbers\n",
    "import re\n",
    "for doc in new_corpus:\n",
    "    i = new_corpus.index(doc)\n",
    "    result = re.sub(r'\\b[\\d]+\\b', '', doc)\n",
    "    new_corpus[i] = result\n",
    "\n",
    "#Remove Stop Words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "tokenized_corpus_idf = [nltk.word_tokenize(doc) for doc in corpus_idf]\n",
    "\n",
    "for doc in tokenized_corpus_idf:\n",
    "    i = tokenized_corpus_idf.index(doc)\n",
    "    result = [word for word in tokenized_corpus_idf[i] if word not in stopwords]\n",
    "    tokenized_corpus_idf[i] = result\n",
    "    \n",
    "#Porter Stemmer for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for docs in tokenized_corpus_idf:\n",
    "    i = tokenized_corpus_idf.index(docs)\n",
    "    for words in docs:\n",
    "        j = tokenized_corpus_idf[i].index(words)\n",
    "        tokenized_corpus_idf[i][j] = ps.stem(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(\n",
    "    use_idf=True,\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)\n",
    "\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(tokenized_corpus_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "    x = np.array(list1) \n",
    "    return np.unique(x)\n",
    "\n",
    "list = unique(tokenized_corpus_idf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "\n",
    "for words in list:\n",
    "    if words in tfidf_vectorizer.get_feature_names():\n",
    "        index = tfidf_vectorizer.get_feature_names().index(words)\n",
    "        dict[words] = tfidf_vectorizer.idf_[index]\n",
    "keys = np.fromiter(dict.keys(), dtype='<U7')\n",
    "vals = np.fromiter(dict.values(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['acd', 'v1', 'anticor', 'patwari', 'gratif', 'trap', 'raid',\n",
       "       'superan', 'suspens', 'pension', 'therefr', 'meanwhi', 'deputi',\n",
       "       '1985', 'superin', '1976', '1987', 'represe', 'thereto', '1998',\n",
       "       '2001', 'age', 'pursuan', 'stood', '2003', 'octob', 'compet',\n",
       "       'februar', 'januari', 'complai', 'juli', 'singl', 'illeg', 'event',\n",
       "       'villag', 'lodg', 'servic', 'ask', 'sought', 'acquit', 'commiss',\n",
       "       'divis', 'reach', 'writ', 'work', 'ie', 'laid', '25', 'prefer'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "feature_arr = np.array(keys)\n",
    "tfidf_sort = np.argsort(vals).flatten()[::-1]\n",
    "\n",
    "top = len(list)/2\n",
    "m = math.ceil(top)\n",
    "top_m = feature_arr[tfidf_sort][:m]\n",
    "top_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.58760044,  7.76808593, 11.05586693, ...,  2.83719159,\n",
       "        8.08823998,  5.35177594])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores_A = bm25A.get_scores(top_m)\n",
    "doc_scores_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q\" = FINDING SCORES FOR q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25B = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 97.27615437,  97.28302863, 110.61634174, ...,  77.07711737,\n",
       "        85.1291916 ,  98.46188324])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores_B = bm25B.get_scores(list)\n",
    "doc_scores_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDING SCORES for q' and q\" and obtain scores for all casedocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = [doc_scores_A + doc_scores_B for i in range(len(doc_scores_A))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([106.86375481, 105.05111457, 121.67220867, ...,  79.91430897,\n",
       "        93.21743158, 103.81365918])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_list = res_list[0]\n",
    "res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SORTING RESULTANT SCORES IN DESCENDING ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = np.argsort(res_list)[::-1]\n",
    "result = [corpus[i] for i in top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([190.35056406, 181.80914462, 177.64910465, ...,  30.0382047 ,\n",
       "        25.76576609,  22.74720282])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = res_list[top_n]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDING FILENAME CORRESPONDING TO SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for item in result:\n",
    "    for filename, content in corpus_dict.items():\n",
    "        if content == item:\n",
    "            output.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = []\n",
    "sstring = '.txt'\n",
    "for item in output:\n",
    "    if item.endswith(sstring): \n",
    "        new_output.append(item[:-(len(sstring))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITING TO A FILE FOR TREC-EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"C:\\\\Users\\\\ASUS\\\\AILA Practice\\\\run-bm25.txt\",\"a\")\n",
    "for i in range(len(corpus)):\n",
    "    f.write(\"AILA_Q49 Q0 {} {} {} Default\\n\".format(new_output[i], (i+1), scores[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2914"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
