{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rel(q,d) = BM25(q',d) + BM25(q\",d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Corpus from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "folder = \"C:\\\\Users\\\\ASUS\\\\AILA Practice\\\\casedocs\"\n",
    "os.chdir(folder)\n",
    "files = glob.glob(\"*.txt\") # Makes a list of all files in folder\n",
    "corpus = []\n",
    "corpus_dict = {}\n",
    "for file1 in files:\n",
    "    with open (file1, 'r') as f:\n",
    "        document = f.read() # Reads document content into a string\n",
    "        corpus.append(document)\n",
    "        corpus_dict[file1] = document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations and stemming using Porter Stemmer and creating tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to Lowercase\n",
    "import nltk\n",
    "def to_lower(text):\n",
    "    return ' '.join([w.lower() for w in nltk.word_tokenize(text)])\n",
    "\n",
    "for doc in new_corpus:\n",
    "    i = new_corpus.index(doc)\n",
    "    result = to_lower(doc)\n",
    "    new_corpus[i] = result\n",
    "    \n",
    "#Remove Punctuations\n",
    "from string import punctuation\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "for doc in new_corpus:\n",
    "    i = new_corpus.index(doc)\n",
    "    result = strip_punctuation(doc)\n",
    "    new_corpus[i] = result\n",
    "\n",
    "#Remove Numbers\n",
    "for doc in new_corpus:\n",
    "    i = new_corpus.index(doc)\n",
    "    result = ''.join(c for c in doc if not c.isdigit())\n",
    "    new_corpus[i] = result\n",
    "\n",
    "#Remove Stop Words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in new_corpus]\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    i = tokenized_corpus.index(doc)\n",
    "    result = [word for word in tokenized_corpus[i] if word not in stopwords]\n",
    "    tokenized_corpus[i] = result\n",
    "    \n",
    "#Porter Stemmer for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for docs in tokenized_corpus:\n",
    "    i = tokenized_corpus.index(docs)\n",
    "    for words in docs:\n",
    "        j = tokenized_corpus[i].index(words)\n",
    "        tokenized_corpus[i][j] = ps.stem(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using rank_bm25 library and running it against tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25A = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q' = FINDING IDF VALUES FOR QUERY AND EXTRACTING TOP 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing casedocs_idf = casedocs + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "folder_idf = \"C:\\\\Users\\\\ASUS\\\\AILA Practice\\\\casedocs_idf\"\n",
    "os.chdir(folder_idf)\n",
    "files_idf = glob.glob(\"*.txt\") # Makes a list of all files in folder\n",
    "corpus_idf = []\n",
    "dict_idf={}\n",
    "for file1 in files_idf:\n",
    "    with open (file1, 'r') as f:\n",
    "        document = f.read() # Reads document content into a string\n",
    "        corpus_idf.append(document)\n",
    "        dict_idf[file1] = document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing it the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to Lowercase\n",
    "import nltk\n",
    "def to_lower(text):\n",
    "    return ' '.join([w.lower() for w in nltk.word_tokenize(text)])\n",
    "\n",
    "for doc in corpus_idf:\n",
    "    i = corpus_idf.index(doc)\n",
    "    result = to_lower(doc)\n",
    "    corpus_idf[i] = result\n",
    "    \n",
    "#Remove Punctuations\n",
    "from string import punctuation\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "for doc in corpus_idf:\n",
    "    i = corpus_idf.index(doc)\n",
    "    result = strip_punctuation(doc)\n",
    "    corpus_idf[i] = result\n",
    "\n",
    "#Remove Numbers\n",
    "for doc in corpus_idf:\n",
    "    i = corpus_idf.index(doc)\n",
    "    result = ''.join(c for c in doc if not c.isdigit())\n",
    "    corpus_idf[i] = result\n",
    "\n",
    "#Remove Stop Words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "tokenized_corpus_idf = [nltk.word_tokenize(doc) for doc in corpus_idf]\n",
    "\n",
    "for doc in tokenized_corpus_idf:\n",
    "    i = tokenized_corpus_idf.index(doc)\n",
    "    result = [word for word in tokenized_corpus_idf[i] if word not in stopwords]\n",
    "    tokenized_corpus_idf[i] = result\n",
    "    \n",
    "#Porter Stemmer for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for docs in tokenized_corpus_idf:\n",
    "    i = tokenized_corpus_idf.index(docs)\n",
    "    for words in docs:\n",
    "        j = tokenized_corpus_idf[i].index(words)\n",
    "        tokenized_corpus_idf[i][j] = ps.stem(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(\n",
    "    use_idf=True,\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)\n",
    "\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(tokenized_corpus_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "    x = np.array(list1) \n",
    "    return np.unique(x)\n",
    "\n",
    "list = unique(tokenized_corpus_idf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "\n",
    "for words in list:\n",
    "    if words in tfidf_vectorizer.get_feature_names():\n",
    "        index = tfidf_vectorizer.get_feature_names().index(words)\n",
    "        dict[words] = tfidf_vectorizer.idf_[index]\n",
    "keys = np.fromiter(dict.keys(), dtype='<U7')\n",
    "vals = np.fromiter(dict.values(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['souveni', 'backgro', 'xxvi', 'derogat', 'expunct', 'demit',\n",
       "       'dispara', 'drama', 'donat', 'resign', 'cabinet', 'gratif',\n",
       "       'bribe', 'trap', 'mine', 'eleven', 'tradit', 'currenc', 'club',\n",
       "       'democra', 'mo', 'vigil', 'remark', 'dw', 'minist', 'welfar',\n",
       "       'figur', 'sudden', 'fund', 'childre', 'tender', 'peculia',\n",
       "       'publish', 'er', 'exhibit', 'credibl', 'highli', 'superin',\n",
       "       'advers', 'featur', 'brief', 'extract', 'besid', 'probabl',\n",
       "       'offer', 'viz', 'rate', 'sum', 'pursuan', 'mark', 'manag',\n",
       "       'testimo', 'arrest', 'recov', 'demand', 'lay', 'led', 'paid',\n",
       "       'substan', 'acquitt', 'rigor', 'award', 'explan', 'februar',\n",
       "       'januari', 'complai', 'toward', 'illeg', 'presenc', 'enter',\n",
       "       'possess', 'defenc', 'april', 'separ', 'end'], dtype='<U7')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "feature_arr = np.array(keys)\n",
    "tfidf_sort = np.argsort(vals).flatten()[::-1]\n",
    "\n",
    "top = len(list)/2\n",
    "m = math.ceil(top)\n",
    "top_m = feature_arr[tfidf_sort][:m]\n",
    "top_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.47169379, 17.4409036 , 47.82054746, ..., 10.50572082,\n",
       "        6.4664513 , 22.23143654])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores_A = bm25A.get_scores(top_m)\n",
    "doc_scores_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q\" = FINDING SCORES FOR q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25B = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109.72465704, 126.80913649, 182.80342353, ..., 113.01081591,\n",
       "       115.26189421, 152.36056835])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores_B = bm25B.get_scores(list)\n",
    "doc_scores_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = [doc_scores_A + doc_scores_B for i in range(len(doc_scores_A))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([116.19635083, 144.25004009, 230.62397099, ..., 123.51653673,\n",
       "       121.72834551, 174.5920049 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_list = res_list[0]\n",
    "res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "top_n = np.argsort(res_list)[::-1][:n]\n",
    "result = [corpus[i] for i in top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for item in result:\n",
    "    for filename, content in corpus_dict.items():\n",
    "        if content == item:\n",
    "            output.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C823.txt',\n",
       " 'C2462.txt',\n",
       " 'C2316.txt',\n",
       " 'C2725.txt',\n",
       " 'C2798.txt',\n",
       " 'C993.txt',\n",
       " 'C2765.txt',\n",
       " 'C2644.txt',\n",
       " 'C2657.txt',\n",
       " 'C2846.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = []\n",
    "sstring = '.txt'\n",
    "for item in output:\n",
    "    if item.endswith(sstring): \n",
    "        new_output.append(item[:-(len(sstring))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C823',\n",
       " 'C2462',\n",
       " 'C2316',\n",
       " 'C2725',\n",
       " 'C2798',\n",
       " 'C993',\n",
       " 'C2765',\n",
       " 'C2644',\n",
       " 'C2657',\n",
       " 'C2846']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREC_EVAL GOLD STANDARD - PRECISION @ 10 CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\ASUS\\\\AILA Practice\\\\trec_goldstd_priorcases.txt') as f:\n",
    "    irrelevant = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "\n",
    "for line in irrelevant:\n",
    "    key = line.partition(' Q0 ')[0]\n",
    "    temp = line.partition(' Q0 ')[2]\n",
    "    val = temp.partition(' 0')[0]\n",
    "    dict.setdefault(key, [])\n",
    "    dict[key].append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_relevant = 0\n",
    "\n",
    "for item in new_output:\n",
    "    if item not in dict['AILA_Q50']:\n",
    "        count_relevant = count_relevant + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
